# Master Documentation Update Workflow
#
# Updates documentation from configured sites using the Content Scraper API.
# Reads configuration from selectors.json and supports caching.

name: Update Documentation

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment'
        required: true
        default: 'prod'
        type: choice
        options:
          - prod
          - dev
      use_cache:
        description: 'Use cache'
        required: true
        default: true
        type: boolean
      unsloth:
        description: 'Update Unsloth docs'
        required: false
        default: false
        type: boolean
      claude-code:
        description: 'Update Claude Code docs'
        required: false
        default: false
        type: boolean
      cursor:
        description: 'Update Cursor docs'
        required: false
        default: false
        type: boolean
      convex:
        description: 'Update Convex docs'
        required: false
        default: false
        type: boolean
      modal:
        description: 'Update Modal docs'
        required: false
        default: false
        type: boolean

jobs:
  update-docs:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Build API URL
        id: api
        run: |
          ENV="${{ inputs.environment }}"
          if [ "$ENV" = "dev" ]; then
            URL_SUFFIX="-dev"
          else
            URL_SUFFIX=""
          fi
          BASE_URL="https://${{ secrets.MODAL_USERNAME }}--content-scraper-api-fastapi-app${URL_SUFFIX}.modal.run"
          echo "base_url=$BASE_URL" >> $GITHUB_OUTPUT
          echo "Using environment: $ENV"
          echo "API URL: $BASE_URL"

      - name: Build scrape requests from selectors.json
        id: build-requests
        run: |
          # Build list of selected sites
          SELECTED_SITES=""
          if [ "${{ inputs.unsloth }}" = "true" ]; then SELECTED_SITES="$SELECTED_SITES unsloth"; fi
          if [ "${{ inputs.claude-code }}" = "true" ]; then SELECTED_SITES="$SELECTED_SITES claude-code"; fi
          if [ "${{ inputs.cursor }}" = "true" ]; then SELECTED_SITES="$SELECTED_SITES cursor"; fi
          if [ "${{ inputs.convex }}" = "true" ]; then SELECTED_SITES="$SELECTED_SITES convex"; fi
          if [ "${{ inputs.modal }}" = "true" ]; then SELECTED_SITES="$SELECTED_SITES modal"; fi

          if [ -z "$SELECTED_SITES" ]; then
            echo "No sites selected. Exiting."
            echo "has_requests=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "Selected sites:$SELECTED_SITES"

          # Build requests JSON from selectors.json
          python3 << 'EOF'
          import json
          import os

          selected = "$SELECTED_SITES".split()

          with open("selectors.json") as f:
              data = json.load(f)

          requests = []
          site_page_map = []  # Track site/page for each request

          for site_name, site in data["sites"].items():
              if site_name not in selected:
                  continue

              selectors = site.get("selectors", {})
              if "copyButton" not in selectors:
                  continue

              method = selectors.get("method", "click_copy")

              for page_name, page_path in site["pages"].items():
                  url = site["baseUrl"] + page_path
                  requests.append({
                      "url": url,
                      "selector": selectors["copyButton"],
                      "method": method
                  })
                  site_page_map.append({"site": site_name, "page": page_name})

          # Write requests to file
          with open("/tmp/requests.json", "w") as f:
              json.dump({"requests": requests, "use_cache": ${{ inputs.use_cache }}}, f)

          # Write site/page map for later
          with open("/tmp/site_page_map.json", "w") as f:
              json.dump(site_page_map, f)

          print(f"Built {len(requests)} scrape requests")
          EOF

          if [ -f /tmp/requests.json ]; then
            echo "has_requests=true" >> $GITHUB_OUTPUT
            cat /tmp/requests.json | jq .
          else
            echo "has_requests=false" >> $GITHUB_OUTPUT
          fi

      - name: Call batch scrape endpoint
        id: scrape
        if: steps.build-requests.outputs.has_requests == 'true'
        run: |
          echo "Calling batch scrape endpoint..."
          echo "Cache enabled: ${{ inputs.use_cache }}"

          response=$(curl -s -X POST \
            "${{ steps.api.outputs.base_url }}/scrape/batch" \
            -H "Content-Type: application/json" \
            -H "Modal-Key: ${{ secrets.MODAL_KEY }}" \
            -H "Modal-Secret: ${{ secrets.MODAL_SECRET }}" \
            -d @/tmp/requests.json)

          echo "$response" > /tmp/response.json

          # Display summary
          echo "Response received!"
          echo "Total: $(echo "$response" | jq -r '.total')"
          echo "Successful: $(echo "$response" | jq -r '.successful')"
          echo "Failed: $(echo "$response" | jq -r '.failed')"
          echo "Cached: $(echo "$response" | jq -r '.cached // 0')"
          echo "Processing time: $(echo "$response" | jq -r '.total_processing_time_seconds')s"

      - name: Save documentation files
        if: steps.build-requests.outputs.has_requests == 'true'
        run: |
          python3 << 'EOF'
          import json
          import os
          from pathlib import Path

          with open("/tmp/response.json") as f:
              response = json.load(f)

          with open("/tmp/site_page_map.json") as f:
              site_page_map = json.load(f)

          results = response.get("results", [])

          for i, result in enumerate(results):
              if i >= len(site_page_map):
                  break

              site = site_page_map[i]["site"]
              page = site_page_map[i]["page"]
              content = result.get("content", "")
              success = result.get("success", False)
              cached = result.get("cached", False)
              time_s = result.get("processing_time_seconds", 0)

              status = "cached" if cached else ("ok" if success else "FAILED")
              print(f"  {site}/{page}: {status} ({time_s:.1f}s, {len(content)} chars)")

              if success and content:
                  docs_dir = Path("docs") / site
                  docs_dir.mkdir(parents=True, exist_ok=True)
                  (docs_dir / f"{page}.md").write_text(content)
          EOF

      - name: Check for changes
        id: check-changes
        run: |
          git diff --quiet docs/ || echo "changes=true" >> $GITHUB_OUTPUT

      - name: Commit and push changes
        if: steps.check-changes.outputs.changes == 'true'
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git add docs/
          git commit -m "chore: update documentation content [automated]"
          git push
